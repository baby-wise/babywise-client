// Cache en memoria para simular respuestas guardadas (sin MongoDB)
// Estructura: { "UID-YYYY-MM-DD": { response, date, prompt, createdAt } }
const llmResponseCache = {};

// FunciÃ³n helper para obtener la fecha actual en formato YYYY-MM-DD
const getCurrentDate = () => {
  const now = new Date();
  return now.toISOString().split('T')[0];
};

// FunciÃ³n helper para generar clave de cache
const getCacheKey = (UID, date) => {
  return `${UID}-${date}`;
};

// FunciÃ³n helper para generar prompt (hardcodeado por ahora)
const generatePrompt = (UID) => {
  const currentDate = getCurrentDate();
  
  // TODO: AquÃ­ se implementarÃ¡ la lÃ³gica real para obtener datos del bebÃ© de la BD
  return `ActÃºa como un asistente experto en cuidado infantil y desarrollo de bebÃ©s. 

BasÃ¡ndome en los datos de monitoreo del bebÃ© del usuario ${UID} para la fecha ${currentDate}, proporciona un resumen personalizado y recomendaciones.

Datos simulados del dÃ­a de hoy:
- Patrones de sueÃ±o: 3 siestas (9:00-10:30, 13:00-14:30, 17:00-18:00), sueÃ±o nocturno de 20:00 a 06:30
- AlimentaciÃ³n: 6 tomas, Ãºltima toma a las 18:30
- Actividad: PerÃ­odos de juego activos entre comidas
- Estado general: Tranquilo, sin episodios de llanto prolongado

Por favor genera un resumen que incluya:
1. ðŸ¼ Estado de alimentaciÃ³n y recomendaciones
2. ðŸ˜´ AnÃ¡lisis de patrones de sueÃ±o
3. ðŸ‘¶ Observaciones sobre desarrollo y bienestar
4. ðŸ’¡ Recomendaciones personalizadas para los prÃ³ximos dÃ­as
5. âš ï¸ Cualquier punto de atenciÃ³n si lo hay

MantÃ©n un tono cÃ¡lido, profesional y tranquilizador para los padres. Responde en espaÃ±ol.`;
};

// FunciÃ³n helper para generar respuesta del LLM usando apifreellm.com
const generateLLMResponse = async (prompt) => {
  try {
    console.log('Llamando a apifreellm.com con prompt:', prompt.substring(0, 100) + '...');
    
    // Llamada a la API de apifreellm.com
    const response = await fetch('https://apifreellm.com/api/chat', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        message: prompt
      })
    });

    if (!response.ok) {
      throw new Error(`API Error: ${response.status} ${response.statusText}`);
    }

    const data = await response.json();
    console.log('Respuesta recibida de apifreellm.com');
    
    // La API devuelve la respuesta en el campo 'response' o 'message'
    return data.response || data.message || data;
    
  } catch (error) {
    console.error('Error llamando a apifreellm.com:', error);
    
    // Fallback a respuesta hardcodeada si falla la API
    console.log('Usando respuesta de fallback');
    return `[Error de conexiÃ³n con LLM] BasÃ¡ndome en la informaciÃ³n proporcionada, aquÃ­ tienes un resumen personalizado para hoy:

ðŸ¼ **AlimentaciÃ³n**: Tu bebÃ© ha mostrado patrones regulares de alimentaciÃ³n hoy.

ðŸ˜´ **SueÃ±o**: Los ciclos de sueÃ±o han sido consistentes con las recomendaciones para su edad.

ðŸ‘¶ **Desarrollo**: ContinÃºa mostrando signos positivos de desarrollo saludable.

ðŸ’¡ **Recomendaciones**: 
- MantÃ©n la rutina actual de alimentaciÃ³n
- Considera ajustar ligeramente los horarios de siesta
- Todo va muy bien, Â¡sigue asÃ­!

Este es un resumen generado automÃ¡ticamente basado en los datos del dÃ­a.`;
  }
};

const getLLMResponseForUser = async (req, res) => {
  try {
    // Debug logging
    console.log('=== DEBUG getLLMResponseForUser ===');
    console.log('req.method:', req.method);
    console.log('req.headers:', req.headers);
    console.log('req.body:', req.body);
    console.log('req.body type:', typeof req.body);
    console.log('req.body keys:', Object.keys(req.body || {}));
    
    const { UID } = req.body || {};
    
    if (!UID) {
      console.log('UID not found in request');
      return res.status(400).json({ error: "UID is required" });
    }

    const currentDate = getCurrentDate();
    const cacheKey = getCacheKey(UID, currentDate);
    
    console.log(`Buscando respuesta LLM para usuario ${UID} en fecha ${currentDate}`);

    // Buscar si ya existe una respuesta en cache para este usuario y fecha
    const existingResponse = llmResponseCache[cacheKey];

    if (existingResponse) {
      console.log(`Respuesta existente encontrada en cache para ${UID}`);
      return res.status(200).json({
        response: existingResponse.response,
        date: existingResponse.date,
        cached: true
      });
    }

    // Si no existe, generar nueva respuesta
    console.log(`Generando nueva respuesta para ${UID}`);
    
    const prompt = generatePrompt(UID);
    const llmResponse = await generateLLMResponse(prompt);
    
    // Guardar en cache
    llmResponseCache[cacheKey] = {
      UID: UID,
      date: currentDate,
      prompt: prompt,
      response: llmResponse,
      createdAt: new Date().toISOString()
    };

    console.log(`Nueva respuesta LLM guardada en cache para ${UID}`);
    console.log(`Cache actual tiene ${Object.keys(llmResponseCache).length} entradas`);

    return res.status(201).json({
      response: llmResponse,
      date: currentDate,
      cached: false
    });

  } catch (error) {
    console.error('Error en getLLMResponseForUser:', error);
    
    return res.status(500).json({ 
      error: "Error interno del servidor al generar respuesta LLM" 
    });
  }
};

export { getLLMResponseForUser };
